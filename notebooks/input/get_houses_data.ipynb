{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f484b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dca1999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/opt/notebooks/input')\n",
    "import real_estate_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ac918b1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "city = 'bern'\n",
    "radius = 4\n",
    "date = '2022-07-09'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2209300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "BRONZE_TABLE_PATH = '/opt/data_lake/bronze/house_prices_raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab46576",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d95a2a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-09 23:20:04,586 [INFO] Getting houses from locarno (1KM around) ...\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Getting houses from {city} ({radius}KM around) ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62869cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the property ids\n",
    "properties_ids = real_estate_utils.get_properties_ids(city, radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b48de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-09 23:20:17,055 [INFO] Searching 48 houses metadata ...\n",
      "2022-07-09 23:20:18,801 [INFO] 7213986 ready.\n",
      "2022-07-09 23:20:18,806 [INFO] 7203448 ready.\n",
      "2022-07-09 23:20:18,809 [INFO] 7204253 ready.\n",
      "2022-07-09 23:20:18,805 [INFO] 7203447 ready.\n",
      "2022-07-09 23:20:19,610 [INFO] 7145725 ready.\n",
      "2022-07-09 23:20:19,610 [INFO] 7165658 ready.\n",
      "2022-07-09 23:20:19,629 [INFO] 7131431 ready.\n",
      "2022-07-09 23:20:19,659 [INFO] 7187137 ready.\n",
      "2022-07-09 23:20:20,772 [INFO] 7187135 ready.\n",
      "2022-07-09 23:20:20,791 [INFO] 7163974 ready.\n",
      "2022-07-09 23:20:20,794 [INFO] 7112827 ready.\n",
      "2022-07-09 23:20:20,795 [INFO] 7110124 ready.\n",
      "2022-07-09 23:20:21,739 [INFO] 6999908 ready.\n",
      "2022-07-09 23:20:21,741 [INFO] 7173867 ready.\n",
      "2022-07-09 23:20:21,738 [INFO] 7093642 ready.\n",
      "2022-07-09 23:20:21,740 [INFO] 7149544 ready.\n",
      "2022-07-09 23:20:22,639 [INFO] 7162928 ready.\n",
      "2022-07-09 23:20:22,639 [INFO] 6667520 ready.\n",
      "2022-07-09 23:20:22,696 [INFO] 7154723 ready.\n",
      "2022-07-09 23:20:22,750 [INFO] 7082624 ready.\n",
      "2022-07-09 23:20:23,534 [INFO] 7158013 ready.\n",
      "2022-07-09 23:20:23,596 [INFO] 7140214 ready.\n",
      "2022-07-09 23:20:23,651 [INFO] 7124524 ready.\n",
      "2022-07-09 23:20:23,887 [INFO] 6741092 ready.\n",
      "2022-07-09 23:20:24,434 [INFO] 7099447 ready.\n",
      "2022-07-09 23:20:24,528 [INFO] 6542290 ready.\n",
      "2022-07-09 23:20:24,614 [INFO] 7048185 ready.\n",
      "2022-07-09 23:20:24,815 [INFO] 6939758 ready.\n",
      "2022-07-09 23:20:25,357 [INFO] 6939695 ready.\n",
      "2022-07-09 23:20:25,439 [INFO] 6939694 ready.\n",
      "2022-07-09 23:20:25,536 [INFO] 5288541 ready.\n",
      "2022-07-09 23:20:25,738 [INFO] 7113460 ready.\n",
      "2022-07-09 23:20:26,256 [INFO] 7113425 ready.\n",
      "2022-07-09 23:20:26,349 [INFO] 7113422 ready.\n",
      "2022-07-09 23:20:26,437 [INFO] 7112473 ready.\n",
      "2022-07-09 23:20:26,642 [INFO] 7034912 ready.\n",
      "2022-07-09 23:20:27,157 [INFO] 6600123 ready.\n",
      "2022-07-09 23:20:27,238 [INFO] 5603975 ready.\n",
      "2022-07-09 23:20:27,333 [INFO] 6920258 ready.\n",
      "2022-07-09 23:20:27,554 [INFO] 6629146 ready.\n",
      "2022-07-09 23:20:28,049 [INFO] 6553008 ready.\n",
      "2022-07-09 23:20:28,175 [INFO] 5786563 ready.\n",
      "2022-07-09 23:20:28,240 [INFO] 7039350 ready.\n",
      "2022-07-09 23:20:28,450 [INFO] 6725766 ready.\n",
      "2022-07-09 23:20:28,950 [INFO] 6449744 ready.\n",
      "2022-07-09 23:20:29,068 [INFO] 4848854 ready.\n",
      "2022-07-09 23:20:29,155 [INFO] 4848835 ready.\n",
      "2022-07-09 23:20:29,335 [INFO] 6921771 ready.\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Searching {len(properties_ids)} houses metadata ...\")\n",
    "\n",
    "# Build a Pandas DataFrame with the responses\n",
    "with ProcessPoolExecutor(max_workers = None) as executor:\n",
    "    try:\n",
    "        results = list(executor.map(\n",
    "            real_estate_utils.parse_property_metadata, \n",
    "            properties_ids\n",
    "        ))\n",
    "\n",
    "        results_total = pd.concat(results)\n",
    "        results_total['date'] = date\n",
    "        results_total['city'] = city\n",
    "        results_total = results_total.dropna(axis='columns', how='all').reset_index(drop=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27a852db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48 entries, 0 to 47\n",
      "Data columns (total 25 columns):\n",
      " #   Column                                            Non-Null Count  Dtype \n",
      "---  ------                                            --------------  ----- \n",
      " 0   property_id                                       48 non-null     object\n",
      " 1   attributes_size_volume                            7 non-null      object\n",
      " 2   attributes_size_number_of_floors                  21 non-null     object\n",
      " 3   attributes_inside_animal_allowed                  15 non-null     object\n",
      " 4   attributes_inside_bathrooms                       5 non-null      object\n",
      " 5   attributes_inside_cellar                          4 non-null      object\n",
      " 6   attributes_technology_dishwasher                  5 non-null      object\n",
      " 7   attributes_technology_cable_tv                    12 non-null     object\n",
      " 8   attributes_outside_balcony                        27 non-null     object\n",
      " 9   attributes_outside_parking                        22 non-null     object\n",
      " 10  attributes_outside_garage                         13 non-null     object\n",
      " 11  attributes_surrounding_distance_shop              16 non-null     object\n",
      " 12  attributes_surrounding_distance_kindergarten      13 non-null     object\n",
      " 13  attributes_surrounding_distance_school_1          15 non-null     object\n",
      " 14  attributes_surrounding_distance_school_2          10 non-null     object\n",
      " 15  attributes_surrounding_distance_public_transport  16 non-null     object\n",
      " 16  attributes_surrounding_distance_motorway          8 non-null      object\n",
      " 17  normalized_price                                  48 non-null     int64 \n",
      " 18  number_of_rooms                                   44 non-null     object\n",
      " 19  surface_property                                  32 non-null     object\n",
      " 20  surface_usable                                    7 non-null      object\n",
      " 21  surface_living                                    40 non-null     object\n",
      " 22  request_response                                  48 non-null     object\n",
      " 23  date                                              48 non-null     object\n",
      " 24  city                                              48 non-null     object\n",
      "dtypes: int64(1), object(24)\n",
      "memory usage: 9.5+ KB\n"
     ]
    }
   ],
   "source": [
    "results_total.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "213104fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-08776f7e-413a-4373-9f69-bcfc12bc2143;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.0.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/1.0.0/delta-core_2.12-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;1.0.0!delta-core_2.12.jar (2021ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4/4.7/antlr4-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4;4.7!antlr4.jar (511ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.7!antlr4-runtime.jar (271ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr-runtime;3.5.2!antlr-runtime.jar (262ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/ST4/4.0.8/ST4-4.0.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#ST4;4.0.8!ST4.jar (266ms)\n",
      "downloading https://repo1.maven.org/maven2/org/abego/treelayout/org.abego.treelayout.core/1.0.3/org.abego.treelayout.core-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.abego.treelayout#org.abego.treelayout.core;1.0.3!org.abego.treelayout.core.jar(bundle) (278ms)\n",
      "downloading https://repo1.maven.org/maven2/org/glassfish/javax.json/1.0.4/javax.json-1.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.glassfish#javax.json;1.0.4!javax.json.jar(bundle) (255ms)\n",
      "downloading https://repo1.maven.org/maven2/com/ibm/icu/icu4j/58.2/icu4j-58.2.jar ...\n",
      "\t[SUCCESSFUL ] com.ibm.icu#icu4j;58.2!icu4j.jar (1859ms)\n",
      ":: resolution report :: resolve 22740ms :: artifacts dl 5738ms\n",
      "\t:: modules in use:\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   8   |   8   |   0   ||   8   |   8   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-08776f7e-413a-4373-9f69-bcfc12bc2143\n",
      "\tconfs: [default]\n",
      "\t8 artifacts copied, 0 already retrieved (15452kB/173ms)\n",
      "22/07/09 23:21:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "builder = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"real-estate-etl\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db380ed0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data is already a DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a Spark DataFrame\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results_total\u001b[38;5;241m=\u001b[39m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults_total\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      3\u001b[0m results_total\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/session.py:658\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSparkSession\u001b[38;5;241m.\u001b[39msetActiveSession(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession)\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, DataFrame):\n\u001b[0;32m--> 658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata is already a DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    661\u001b[0m     schema \u001b[38;5;241m=\u001b[39m _parse_datatype_string(schema)\n",
      "\u001b[0;31mTypeError\u001b[0m: data is already a DataFrame"
     ]
    }
   ],
   "source": [
    "# Create a Spark DataFrame\n",
    "results_total=spark.createDataFrame(results_total) \n",
    "results_total.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4245e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Store it on bronze Delta Table\n",
    "(\n",
    "    results_total\n",
    "    .write\n",
    "    .format('delta')\n",
    "    .partitionBy(['date', 'city'])\n",
    "    .mode('overwrite')\n",
    "    .option('mergeSchema', 'true')\n",
    "    .option('replaceWhere', f\"date=='{date}' and city=='{city}'\")\n",
    "    .save(BRONZE_TABLE_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad131996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Vacuum table\n",
    "# delta_table = DeltaTable.forPath(spark, BRONZE_TABLE_PATH)\n",
    "# delta_table.vacuum()\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
